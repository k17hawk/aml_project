# Start from JDK base image (build + runtime)
FROM eclipse-temurin:17-jdk-jammy

# Versions
ARG SPARK_VERSION=3.5.0
ARG HADOOP_VERSION=3.3.6
ARG SCALA_VERSION=2.13
ARG KUBERNETES_CLIENT_VERSION=29.0.0
# ARG MONGODB_VERSION=8.0
# ARG MONGOSH_VERSION=2.4.2
ARG PYTHON_VERSION=3.11
ARG ODBC_DRIVER_VERSION=18
ARG RETRY_COUNT=5
ARG RETRY_DELAY=10

# Install system dependencies
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    curl \
    procps \
    gnupg \
    dos2unix \
    unixodbc-dev \
    software-properties-common \
    && \
    add-apt-repository -y ppa:deadsnakes/ppa && \
    apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-venv python${PYTHON_VERSION}-dev && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 && \
    curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} && \
    rm -rf /var/lib/apt/lists/*

# Environment variables

ENV SPARK_HOME=/opt/spark \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    JAVA_HOME=/opt/java/openjdk \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH="/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip" \
    PATH="${JAVA_HOME}/bin:/opt/hadoop/bin:/opt/spark/bin:/opt/venv/bin:${PATH}"

# Install MongoDB
# RUN curl -fsSL https://www.mongodb.org/static/pgp/server-${MONGODB_VERSION}.asc | gpg --dearmor -o /usr/share/keyrings/mongodb.gpg && \
#     echo "deb [signed-by=/usr/share/keyrings/mongodb.gpg] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/${MONGODB_VERSION} multiverse" | tee /etc/apt/sources.list.d/mongodb-org-${MONGODB_VERSION}.list && \
#     apt-get update && \
#     apt-get install -y mongodb-org-shell mongodb-mongosh && \
#     rm -rf /var/lib/apt/lists/*

# Install ODBC driver
# RUN curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > /etc/apt/trusted.gpg.d/microsoft.gpg && \
#     echo "deb [arch=amd64] https://packages.microsoft.com/ubuntu/22.04/prod jammy main" > /etc/apt/sources.list.d/mssql-release.list && \
#     apt-get update && \
#     ACCEPT_EULA=Y apt-get install -y msodbcsql${ODBC_DRIVER_VERSION} && \
#     apt-get clean && \
#     rm -rf /var/lib/apt/lists/* && \
#     ln -s /opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.so /usr/lib/x86_64-linux-gnu/




RUN mkdir -p /opt && \
    (for i in $(seq 1 ${RETRY_COUNT}); do \
        echo "Attempt $i/${RETRY_COUNT}: Downloading Hadoop ${HADOOP_VERSION}..."; \
        if curl -L --retry ${RETRY_COUNT} --retry-delay ${RETRY_DELAY} \
            https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \
            -o /tmp/hadoop.tar.gz; then \
            \
            echo "Download completed, verifying checksum..."; \
            if ! tar -tzf /tmp/hadoop.tar.gz >/dev/null; then \
                echo "ERROR: Downloaded file is corrupt"; \
                rm -f /tmp/hadoop.tar.gz; \
                continue; \
            fi; \
            \
            echo "Extracting Hadoop..."; \
            if tar -xzf /tmp/hadoop.tar.gz -C /opt; then \
                rm -f /tmp/hadoop.tar.gz; \
                \
                # Verify extraction
                if [ -d "/opt/hadoop-${HADOOP_VERSION}" ]; then \
                    echo "Hadoop successfully extracted"; \
                    break; \
                else \
                    echo "ERROR: Extraction failed - directory not found"; \
                    continue; \
                fi; \
            else \
                echo "ERROR: Extraction failed"; \
                continue; \
            fi; \
        else \
            echo "ERROR: Download failed"; \
        fi; \
        \
        [ $i -eq ${RETRY_COUNT} ] && echo "FATAL: All retries exhausted" && exit 1; \
        sleep ${RETRY_DELAY}; \
    done) && \
    \
# Post-installation setup
ln -sfn /opt/hadoop-${HADOOP_VERSION} /opt/hadoop && \
    chown -R root:root /opt/hadoop-${HADOOP_VERSION} && \
    rm -rf /opt/hadoop/share/doc && \
    find /opt/hadoop -name "*-sources.jar" -delete && \
    \
    # Verify installation
    if [ ! -f "/opt/hadoop/bin/hadoop" ]; then \
        echo "ERROR: Hadoop installation verification failed"; \
        exit 1; \
    fi

# Install Spark with conf-org approach
RUN (for i in $(seq 1 ${RETRY_COUNT}); do \
        curl -L --retry ${RETRY_COUNT} --retry-delay ${RETRY_DELAY} \
        https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3-scala${SCALA_VERSION}.tgz | \
        tar -xz -C /opt && \
        mv /opt/spark-${SPARK_VERSION}-bin-hadoop3-scala${SCALA_VERSION} /opt/spark-${SPARK_VERSION} && \
        break || \
        (echo "Spark download failed $i/${RETRY_COUNT}, retrying..." && \
         sleep ${RETRY_DELAY} && \
         rm -rf /opt/spark-* && \
         [ $i -eq ${RETRY_COUNT} ] && exit 1); \
    done) && \
    ln -s /opt/spark-${SPARK_VERSION} /opt/spark && \
    # Create conf-org and move original configs there \
    mkdir -p /opt/spark/conf-org && \
    mv /opt/spark/conf/* /opt/spark/conf-org/ 2>/dev/null || true && \
    # Setup spark-env.sh in conf-org \
    cp /opt/spark/conf-org/spark-env.sh.template /opt/spark/conf-org/spark-env.sh && \
    echo "export SPARK_DIST_CLASSPATH=\$(/opt/hadoop/bin/hadoop classpath)" >> /opt/spark/conf-org/spark-env.sh && \
    echo "export SPARK_EXTRA_CLASSPATH=\$(/opt/hadoop/bin/hadoop classpath)" >> /opt/spark/conf-org/spark-env.sh && \
    # Cleanup \
    rm -rf /opt/spark/examples /opt/spark/data

RUN cp /opt/spark/conf-org/spark-env.sh.template /opt/spark/conf-org/spark-env.sh && \
    echo "export SPARK_DIST_CLASSPATH=\$(/opt/hadoop/bin/hadoop classpath)" >> /opt/spark/conf-org/spark-env.sh && \
    echo "export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop" >> /opt/spark/conf-org/spark-env.sh

# Create symlink for Spark's expected Java path
RUN mkdir -p /usr/lib/jvm && \
    ln -s ${JAVA_HOME} /usr/lib/jvm/java-17-openjdk-amd64

# Set up Python environment
RUN python${PYTHON_VERSION} -m venv /opt/venv && \
    /opt/venv/bin/pip install --upgrade pip

WORKDIR /app  
COPY . /app   

RUN if [ -f requirements.txt ]; then /opt/venv/bin/pip install --no-cache-dir -r requirements.txt; fi && \
    find /opt/venv -name "*.pyc" -delete && \
    chmod +x /app/entrypoint.sh && dos2unix /app/entrypoint.sh

# Ensure proper permissions and configurations
RUN chmod -R a+rwx /opt/spark && \
    mkdir -p /opt/spark/conf && \
    touch /opt/spark/conf/spark-defaults.conf && \
    echo "spark.driver.extraJavaOptions -Dio.netty.tryReflectionSetAccessible=true" >> /opt/spark/conf/spark-defaults.conf && \
    echo "spark.executor.extraJavaOptions -Dio.netty.tryReflectionSetAccessible=true" >> /opt/spark/conf/spark-defaults.conf && \
    echo "source /opt/spark/conf-org/spark-env.sh" >> /etc/profile

    # Critical Spark configs
RUN echo "spark.ui.port=4040" >> /opt/spark/conf/spark-defaults.conf && \
echo "spark.driver.bindAddress=0.0.0.0" >> /opt/spark/conf/spark-defaults.conf

EXPOSE 4040 
EXPOSE 7078 
EXPOSE 5000
ENTRYPOINT ["/app/entrypoint.sh"]
CMD ["pyspark"]