{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_session\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session =  SparkSession.builder\\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .appName('AML_project')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+----------------+--------+----------------+-----------------+--------------------+----------------------+------------+-------------+--------------------+\n",
      "|    Time|      Date|Sender_account|Receiver_account|  Amount|Payment_currency|Received_currency|Sender_bank_location|Receiver_bank_location|Payment_type|Is_laundering|     Laundering_type|\n",
      "+--------+----------+--------------+----------------+--------+----------------+-----------------+--------------------+----------------------+------------+-------------+--------------------+\n",
      "|10:35:19|2022-10-07|    8724731955|      2769355426| 1459.15|       UK pounds|        UK pounds|                  UK|                    UK|Cash Deposit|            0|Normal_Cash_Deposits|\n",
      "|10:35:20|2022-10-07|    1491989064|      8401255335| 6019.64|       UK pounds|           Dirham|                  UK|                   UAE|Cross-border|            0|      Normal_Fan_Out|\n",
      "|10:35:20|2022-10-07|     287305149|      4404767002|14328.44|       UK pounds|        UK pounds|                  UK|                    UK|      Cheque|            0|Normal_Small_Fan_Out|\n",
      "|10:35:21|2022-10-07|    5376652437|      9600420220| 11895.0|       UK pounds|        UK pounds|                  UK|                    UK|         ACH|            0|       Normal_Fan_In|\n",
      "|10:35:21|2022-10-07|    9614186178|      3803336972|  115.25|       UK pounds|        UK pounds|                  UK|                    UK|Cash Deposit|            0|Normal_Cash_Deposits|\n",
      "|10:35:21|2022-10-07|    8974559268|      3143547511| 5130.99|       UK pounds|        UK pounds|                  UK|                    UK|         ACH|            0|        Normal_Group|\n",
      "|10:35:23|2022-10-07|     980191499|      8577635959|12176.52|       UK pounds|        UK pounds|                  UK|                    UK|         ACH|            0|Normal_Small_Fan_Out|\n",
      "|10:35:23|2022-10-07|    8057793308|      9350896213|    56.9|       UK pounds|        UK pounds|                  UK|                    UK| Credit card|            0|Normal_Small_Fan_Out|\n",
      "|10:35:26|2022-10-07|    6116657264|       656192169| 4738.45|       UK pounds|        UK pounds|                  UK|                    UK|      Cheque|            0|      Normal_Fan_Out|\n",
      "|10:35:29|2022-10-07|    7421451752|      2755709071| 5883.87|    Indian rupee|        UK pounds|                  UK|                    UK| Credit card|            0|      Normal_Fan_Out|\n",
      "|10:35:31|2022-10-07|    5119661534|      9734073275| 2342.31|       UK pounds|        UK pounds|                  UK|                    UK|  Debit card|            0|Normal_Small_Fan_Out|\n",
      "|10:35:34|2022-10-07|    5606024775|      8646193759| 1239.61|       UK pounds|        UK pounds|                  UK|                    UK|Cash Deposit|            0|Normal_Cash_Deposits|\n",
      "|10:35:34|2022-10-07|    1405792399|      5109623450|16555.31|       UK pounds|  Pakistani rupee|                  UK|                    UK| Credit card|            0|       Normal_Fan_In|\n",
      "|10:35:37|2022-10-07|    2188890133|      3938416782|15459.46|       UK pounds|        UK pounds|                  UK|                    UK|      Cheque|            0|Normal_Small_Fan_Out|\n",
      "|10:35:37|2022-10-07|    6715177555|      4460925916|  586.28|       UK pounds|        UK pounds|                  UK|                    UK|      Cheque|            0|Normal_Small_Fan_Out|\n",
      "|10:35:37|2022-10-07|    7017008854|      3714297114| 1971.15|       UK pounds|             Euro|                  UK|                 Spain|Cross-border|            0|Normal_Small_Fan_Out|\n",
      "|10:35:38|2022-10-07|    2047410771|      6646502650| 8110.72|       UK pounds|             Euro|                  UK|                France|Cross-border|            0|       Normal_Fan_In|\n",
      "|10:35:41|2022-10-07|    1470504995|      3347268325|  1190.3|       UK pounds|        UK pounds|                  UK|                    UK| Credit card|            0|        Normal_Group|\n",
      "|10:35:43|2022-10-07|    4473647189|      7892884492|11957.87|       UK pounds|        US dollar|                  UK|                   USA|Cross-border|            0|       Normal_Fan_In|\n",
      "|10:35:44|2022-10-07|    8184612956|      5628674969|10529.75|       UK pounds|        UK pounds|                  UK|                    UK| Credit card|            0|       Normal_Fan_In|\n",
      "+--------+----------+--------------+----------------+--------+----------------+-----------------+--------------------+----------------------+------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.csv(\"../Data/SAML-D.csv\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, to_date\n",
    "\n",
    "df = df.withColumn(\"Date\", to_date(df[\"Date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.withColumn(\"year\", year(df[\"Date\"])) \\\n",
    "       .withColumn(\"month\", month(df[\"Date\"])) \\\n",
    "       .withColumn(\"day\", dayofmonth(df[\"Date\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,hour,minute,second \n",
    "\n",
    "df = df.withColumn(\"Time\", to_timestamp(df[\"Time\"], \"HH:mm:ss\"))\n",
    "\n",
    "df = df.withColumn(\"hour\", hour(df[\"Time\"])) \\\n",
    "       .withColumn(\"minute\", minute(df[\"Time\"])) \\\n",
    "       .withColumn(\"second\", second(df[\"Time\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 7603866 rows\n",
      "Testing Set: 1900986 rows\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training Set: {train_df.count()} rows\")\n",
    "print(f\"Testing Set: {test_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "numeric_cols = ['Is_laundering']\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    train_df = train_df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(col_name, col(col_name).cast(\"double\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
