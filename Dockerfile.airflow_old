# Use Ubuntu 22.04 as the base image
FROM ubuntu:22.04

# Set environment variables to prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# Set environment variables for Java, Airflow, and PySpark
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
ENV AIRFLOW_HOME="/app/airflow"
ENV AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=1000
ENV AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__CORE__DAGS_FOLDER="/app/dags"
ENV PYTHONPATH="/app/src"
# Switch to root user
USER root

# Update system and install required dependencies
RUN apt-get update -y && \
    apt-get upgrade -y && \
    apt-get install -y \
    openjdk-8-jdk \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    gcc \
    libssl-dev \
    libffi-dev \
    wget \
    curl \
    unzip \
    software-properties-common \
    tzdata \
    cmake \
    git \
    libabsl-dev && \
    ln -fs /usr/share/zoneinfo/$TZ /etc/localtime && \
    dpkg-reconfigure --frontend noninteractive tzdata && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set Java environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"
RUN ln -s /usr/bin/python3 /usr/bin/python
# Install Hadoop (needed for Spark)
RUN curl -O https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz && \
    tar -xvzf hadoop-3.3.6.tar.gz && \
    mv hadoop-3.3.6 /opt/hadoop && \
    rm hadoop-3.3.6.tar.gz


# Install Spark
RUN curl -O https://archive.apache.org/dist/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz && \
    tar -xvzf spark-3.5.5-bin-hadoop3.tgz && \
    mv spark-3.5.5-bin-hadoop3 /opt/spark && \
    rm spark-3.5.5-bin-hadoop3.tgz

# Set environment variables for Spark and Hadoop
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV HADOOP_HOME=/opt/hadoop
ENV PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

RUN echo 'export SPARK_DIST_CLASSPATH="$(hadoop classpath)"' >> ~/.bashrc
# Install Microsoft ODBC Drivers
RUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - && \
 curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list > /etc/apt/sources.list.d/mssql-release.list && \
 apt-get update && \
 ACCEPT_EULA=Y apt-get install -y msodbcsql18 mssql-tools unixodbc-dev && \
 echo 'export PATH="$PATH:/opt/mssql-tools/bin"' >> ~/.bashrc


 #  ZooKeeper (version 3.9.2)
RUN wget -qO - https://archive.apache.org/dist/zookeeper/zookeeper-3.9.2/apache-zookeeper-3.9.2-bin.tar.gz | tar xz -C /opt

RUN ln -s /opt/zookeeper-3.7.0 /opt/zookeeper
ENV ZK_HOME /opt/zookeeper
ENV PATH $PATH:$ZK_HOME/bin

# Kafka (version 2.8.0)
RUN wget -qO - https://archive.apache.org/dist/kafka/2.8.0/kafka_2.13-2.8.0.tgz | tar xz -C /opt
RUN ln -s /opt/kafka_2.13-2.8.0 /opt/kafka
ENV KAFKA_HOME /opt/kafka
ENV PATH $PATH:$KAFKA_HOME/bin

# Expose ports for ZooKeeper and Kafka
EXPOSE 2181 9092

# Set the default command (Kafka & Zookeeper start)
CMD ["bash", "-c", "zookeeper-server-start.sh /opt/zookeeper/config/zookeeper.properties & kafka-server-start.sh /opt/kafka/config/server.properties"]


# Create app directory
RUN mkdir /app

# Copy files to the container
COPY . /app/

# Set working directory
WORKDIR /app/

# Install Python dependencies using Airflow's constraints file
ARG AIRFLOW_VERSION=2.10.5
ARG PYTHON_VERSION=3.11
# ARG CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
# RUN pip3 install --no-cache-dir "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
# RUN pip3 install --no-cache-dir -r requirements.txt
RUN pip3 install  --no-cache-dir  -r requirements.txt 
# Set permissions for the start script
RUN chmod +x start.sh

# Set entrypoint to the start script
ENTRYPOINT ["/app/start.sh"]